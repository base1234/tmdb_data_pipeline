# TMDB Data Pipeline

This project builds a simple end-to-end data pipeline using TMDB's movie dataset. It includes data ingestion, transformation, and modeling using Python, PySpark, Airflow, and DBT. The pipeline processes raw JSON files into structured Parquet files and applies transformations to make them analytics-ready.

---

## ğŸ› ï¸ Tools & Technologies Used

- Python
- Airflow
- PySpark
- DBT (Data Build Tool)
- DuckDB (Local Data Warehouse)

---

## ğŸ“ Project Structure
```
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow
â”‚Â Â  â”œâ”€â”€ airflow.cfg
â”‚Â Â  â”œâ”€â”€ airflow.db
â”‚Â Â  â”œâ”€â”€ dags
â”‚Â Â  â”œâ”€â”€ logs
â”‚Â Â  â””â”€â”€ simple_auth_manager_passwords.json.generated
â”œâ”€â”€ artifacts
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ processed
â”‚Â Â  â”œâ”€â”€ raw
â”‚Â Â  â””â”€â”€ warehouse
â”œâ”€â”€ dbt_tmdb
â”‚Â Â  â”œâ”€â”€ dbt_project.yml
â”‚Â Â  â”œâ”€â”€ logs
â”‚Â Â  â”œâ”€â”€ macros
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ seeds
â”‚Â Â  â”œâ”€â”€ snapshots
â”‚Â Â  â””â”€â”€ target
â”œâ”€â”€ docker
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ docker-compose.yml
â”œâ”€â”€ logs
â”‚Â Â  â””â”€â”€ dbt.log
â”œâ”€â”€ moviedashboard.py
â”œâ”€â”€ my_script.py
â”œâ”€â”€ notebooks
â”‚Â Â  â””â”€â”€ analyze_movies.ipynb
â”œâ”€â”€ parquetcheck.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”œâ”€â”€ artifacts
â”‚Â Â  â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ fetch_genres.py
â”‚Â Â  â”œâ”€â”€ flatten_json.py
â”‚Â Â  â”œâ”€â”€ ingest_tmdb.py
â”‚Â Â  â”œâ”€â”€ inspect_schema.py
â”‚Â Â  â”œâ”€â”€ print_paths.py
â”‚Â Â  â”œâ”€â”€ run_etl_with_validation.py
â”‚Â Â  â”œâ”€â”€ spark_etl.py
â”‚Â Â  â””â”€â”€ utils.py
â”œâ”€â”€ setup.sh
â”œâ”€â”€ spark-4.0.0-bin-hadoop3.tgz.1
â”œâ”€â”€ streamlit-venv
â”‚Â Â  â”œâ”€â”€ bin
â”‚Â Â  â”œâ”€â”€ etc
â”‚Â Â  â”œâ”€â”€ include
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ lib64 -> lib
â”‚Â Â  â”œâ”€â”€ pyvenv.cfg
â”‚Â Â  â””â”€â”€ share
â”œâ”€â”€ superset
â”‚Â Â  â”œâ”€â”€ create_admin.py
â”‚Â Â  â”œâ”€â”€ dashboard_config.json
â”‚Â Â  â””â”€â”€ superset_config
â”œâ”€â”€ superset-venv
â”‚Â Â  â”œâ”€â”€ bin
â”‚Â Â  â”œâ”€â”€ include
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ lib64 -> lib
â”‚Â Â  â””â”€â”€ pyvenv.cfg
â”œâ”€â”€ testduckdbconnection.py
â”œâ”€â”€ tmdb.duckdb
â””â”€â”€ venv
    â”œâ”€â”€ bin
    â”œâ”€â”€ include
    â”œâ”€â”€ lib
    â”œâ”€â”€ lib64 -> lib
    â”œâ”€â”€ pyvenv.cfg
    â””â”€â”€ share
```
---

## âš™ï¸ Airflow DAG Overview

`tmdb_pipeline.py` DAG performs the following steps:

1. `fetch_genres`: Fetches genre metadata from the TMDB API.
2. `ingest_movies`: Ingests popular movie data and stores it in JSON.
3. `run_spark_etl`: Reads raw JSONs and flattens/explodes the data using PySpark.
4. `run_dbt`: Seeds DuckDB with static files and runs transformation models.

---

## ğŸ§ª Manual Testing (Before Automation)

You can manually test scripts using:
```bash
python scripts/fetch_genres.py
python scripts/ingest_tmdb.py
spark-submit scripts/spark_etl.py
dbt seed
dbt run
```

---

## ğŸ“Œ Airflow Setup & Execution

1. **Create a virtual environment**
```bash
python -m venv venv
source venv/bin/activate
```

2. **Install Airflow and requirements**
```bash
pip install apache-airflow
```

3. **Create Airflow structure**
```bash
mkdir airflow airflow/dags
```

4. **Add your DAG to `airflow/dags/tmdb_pipeline.py`**

5. **Initialize Airflow DB**
```bash
airflow db init
```

6. **Run Airflow webserver**
```bash
airflow standalone
```

---

## âš¡ PySpark Script (ETL)

`spark_etl.py`:
- Reads raw JSON
- Explodes `results`
- Normalizes fields
- Saves as Parquet to `data/processed`

---

## ğŸ§± DBT Setup

1. Initialize DBT project:
```bash
cd dbt_tmdb
dbt init .
```

2. Configure `profiles.yml` to use DuckDB.

3. Define staging and modeling SQLs inside `models/`.

4. Seed & run models:
```bash
dbt seed
dbt run
```

---

## ğŸ’¡ Optional Enhancements

- Add Superset or Streamlit dashboard for final visualizations
- Schedule DAG with a weekly interval
- Store raw/processed data in cloud storage (e.g., GCS or S3)


# ğŸ¬ TMDB Movie Dashboard

An interactive movie analytics dashboard powered by **Streamlit** and **DuckDB**. Visualize and explore TMDB movie metadata by genre, rating, year, and popularity.

---

## ğŸš€ Features

- âš¡ Fast, local SQL queries using **DuckDB**
- ğŸ¨ Built with **Streamlit** (dark mode)
- ğŸ“ˆ KPIs for average rating, popularity, and vote count
- ğŸ“Š Visuals:
  - Scatter plot: Popularity vs. Vote Count
  - Bar chart: Average Rating by Genre
  - Bar chart: Movie Count by Genre
  - Line chart: Yearly Average Ratings
- ğŸ§© Interactive filters by genre and release year
- ğŸ“‚ Expandable raw movie data view
- ğŸ” Deduplication logic for movies with multiple genres

---

## ğŸ—‚ï¸ Database Schema

Ensure your `tmdb.duckdb` file contains the following tables:

| Table Name          | Description                              |
|---------------------|------------------------------------------|
| `genre_lookup`      | Maps `genre_id` to `genre_name`          |
| `genres`            | Same as above, used for cross-checking   |
| `staging_movies`    | Raw TMDB movie data                      |
| `top_genres`        | Pre-aggregated genre stats               |
| `top_movies`        | Top movies with vote averages            |
| `yearly_avg_ratings`| Average ratings aggregated by year       |

### `staging_movies` key columns:
| Column            | Type    |
|-------------------|---------|
| `movie_id`        | BIGINT  |
| `title`           | VARCHAR |
| `release_date`    | VARCHAR |
| `vote_average`    | DOUBLE  |
| `vote_count`      | BIGINT  |
| `popularity`      | DOUBLE  |
| `genre_id`        | BIGINT  |

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repository
```bash
git clone <your-repo-url>
cd tmdb_data_pipeline
```

### 2. Create & activate a virtual environment
```bash
python3 -m venv streamlit-venv
source streamlit-venv/bin/activate   # Windows: .\streamlit-venv\Scripts\activate
```

### 3. Install required dependencies
```bash
pip install streamlit duckdb pandas plotly
```

### 4. Verify your DuckDB database
Ensure `tmdb.duckdb` exists in the project root and contains the required tables.

### 5. Run the dashboard
```bash
streamlit run moviedashboard.py
```

---

## âš™ï¸ Release Year Parsing Logic

To avoid conversion errors when extracting years from poorly formatted dates, this logic is applied:

```sql
CASE 
  WHEN length(release_date) >= 4 
   AND substr(release_date, 1, 4) GLOB '[0-9][0-9][0-9][0-9]'
  THEN CAST(substr(release_date, 1, 4) AS INTEGER)
  ELSE NULL
END AS release_year
```

---

## ğŸ’¡ Use Cases

- Discover which genres dominate in popularity or rating
- Analyze trends over years for movie ratings
- Compare audience vote volume vs. perceived popularity
- Monitor newly released movies and how they perform

---

## ğŸ“¸ Preview

> [Insert screenshot or GIF of dashboard here if hosting on GitHub]

---

## ğŸ“Œ Notes

- One movie can belong to multiple genres (join logic accounts for this)
- The dashboard uses deduplication logic where needed for KPIs and trends
- Visual layout is structured into sections: KPIs, filters, plots, and raw data

---

## ğŸ“„ License

MIT License (add full license text here if open-sourcing)

---

## ğŸ™‹â€â™‚ï¸ Contributing

PRs are welcome. For major changes, please open an issue first to discuss what youâ€™d like to change or enhance.

